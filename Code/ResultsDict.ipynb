{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Creating dictionaries with infos on each user, for each dataset </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from time import time\n",
    "from networkx.algorithms.centrality import closeness_centrality, betweenness_centrality, katz_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test_rtu' # wcano, russian_rtu, russian_rtid, weibo_rtu, weibo_rtid, tdn10, tdn11, tdnT, test_rtu, test_rtid\n",
    "cascade = False\n",
    "\n",
    "\n",
    "# wcano\n",
    "if dataset == 'wcano':\n",
    "    data_path = \"../Datasets/WCano/wcano_tronc.txt\"\n",
    "    out_path = \"../Datasets/WCano/stats/\"\n",
    "    RTU = False\n",
    "    truegraph = False\n",
    "\n",
    "# russian\n",
    "elif dataset == 'russian_rtu':\n",
    "    data_path = \"../Datasets/russian_election_2018/russian_election_2018_rtu.txt\"\n",
    "    out_path = \"../Datasets/russian_election_2018/results/\"\n",
    "    RTU = True\n",
    "    truegraph = False\n",
    "elif dataset == 'russian_rtid':\n",
    "    data_path = \"../Datasets/russian_election_2018/russian_election_2018_rtid.txt\"\n",
    "    out_path = \"../Datasets/russian_election_2018/results/\"\n",
    "    RTU = False\n",
    "    truegraph = False\n",
    "\n",
    "# weibo\n",
    "elif dataset == 'weibo_rtu':\n",
    "    data_path = \"../Datasets/influence_locality/total_rtu.txt\"\n",
    "    out_path = \"../Datasets/influence_locality/results/\"\n",
    "    RTU = True\n",
    "    truegraph = False\n",
    "elif dataset == 'weibo_rtid':\n",
    "    data_path = \"../Datasets/influence_locality/total_rtid.txt\"\n",
    "    out_path = \"../Datasets/influence_locality/results/\"\n",
    "    RTU = False\n",
    "    truegraph = False\n",
    "\n",
    "# tdn\n",
    "elif dataset == 'tdn10':\n",
    "    data_path = \"../Datasets/twitter_dynamic_net/tweets2010_clean.txt\"\n",
    "    out_path = \"../Datasets/twitter_dynamic_net/results2010/\"\n",
    "    RTU = True\n",
    "    truegraph = False\n",
    "elif dataset == 'tdn11':\n",
    "    data_path = \"../Datasets/twitter_dynamic_net/tweets2011_clean.txt\"\n",
    "    out_path = \"../Datasets/twitter_dynamic_net/results2011/\"\n",
    "    RTU = True\n",
    "    truegraph = False\n",
    "elif dataset == 'tdnT':\n",
    "    adjacency_list = \"../Datasets/twitter_dynamic_net/adjacency_list.txt\"\n",
    "    out_path = \"../Datasets/twitter_dynamic_net/truegraph_results/\"\n",
    "    RTU = False\n",
    "    truegraph = True\n",
    "\n",
    "# test\n",
    "elif dataset == 'test_rtu':\n",
    "    data_path = \"../Datasets/test/test_rtu.txt\"\n",
    "    out_path = \"../Datasets/test/results/\"\n",
    "    RTU = True\n",
    "    truegraph = False\n",
    "elif dataset == 'test_rtid':\n",
    "    data_path = \"../Datasets/test/test_rtid.txt\"\n",
    "    out_path = \"../Datasets/test/results/\"\n",
    "    RTU = False\n",
    "    truegraph = False\n",
    "    \n",
    "else:\n",
    "    print(\"Non existing dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit `out_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RTU:\n",
    "    out_path += \"rtu/\"\n",
    "elif cascade:\n",
    "    out_path += \"cascade/\"\n",
    "else:\n",
    "    if truegraph:\n",
    "        out_path += \"truegraph/\"\n",
    "    else:\n",
    "        out_path += \"rtid/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get $\\lambda, \\mu, \\nu$\n",
    "**Important :** if we don't know the author of some RTid, the reposting user is assumed to be the author. In this case we increase his/her $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tps ex :  0.0\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "users = set()\n",
    "Lambda, Mu, Nu = dict(), dict(), dict()\n",
    "tweets = open(data_path, 'r')\n",
    "\n",
    "\n",
    "#--- cas RTu ---#\n",
    "if RTU:\n",
    "    \n",
    "    # parcourt tweets\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        tweet = tweet.split()\n",
    "        uid, rtu = int(tweet[2]), int(tweet[-1])\n",
    "\n",
    "        # si user non connu on crée des nouvelles entrées de dictionnaire\n",
    "        if uid not in users:\n",
    "            users.add(uid)\n",
    "            Lambda[uid], Mu[uid], Nu[uid] = 0, 0, 0\n",
    "\n",
    "        # si tweet original update nb_tweets\n",
    "        if rtu == -1:\n",
    "            Lambda[uid] += 1\n",
    "\n",
    "        # si retweet update nb_retweets et nb_retweeted, ajoute rtu à users\n",
    "        else:\n",
    "            Mu[uid] += 1\n",
    "            if rtu not in users:\n",
    "                users.add(rtu)\n",
    "                Lambda[rtu], Mu[rtu], Nu[rtu] = 0, 0, 1\n",
    "            else:\n",
    "                Nu[rtu] += 1\n",
    "            G.add_edge(rtu, uid)    \n",
    "                \n",
    "        # on enregistre le ts du 1er tweet et celui du dernier tweet (update à chaque étape car tweets pas forcément pas ordre chrono)\n",
    "        ts = int(tweet[1])\n",
    "        if i==0:\n",
    "            first_ts = ts\n",
    "            last_ts = ts\n",
    "        elif ts < first_ts:\n",
    "            first_ts = ts\n",
    "        elif ts > last_ts:\n",
    "            last_ts = ts\n",
    "\n",
    "            \n",
    "#--- cas RTid ---#       \n",
    "else:\n",
    "    \n",
    "    # si cascade on doit recréer le LastPublisher dict\n",
    "    if cascade:\n",
    "        LastPublisher = dict()\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            twid, uid = int(tweet[0]), int(tweet[2])\n",
    "            LastPublisher[twid] = uid\n",
    "            # si user non connu on crée des nouvelles entrées de dictionnaire\n",
    "            if uid not in users:\n",
    "                users.add(uid)\n",
    "                Lambda[uid], Mu[uid], Nu[uid] = 0, 0, 0\n",
    "            \n",
    "    # sinon on recrée le author dict\n",
    "    else:\n",
    "        Author = dict()\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            twid, uid = int(tweet[0]), int(tweet[2])\n",
    "            Author[twid] = uid\n",
    "            # si user non connu on crée des nouvelles entrées de dictionnaire\n",
    "            if uid not in users:\n",
    "                users.add(uid)\n",
    "                Lambda[uid], Mu[uid], Nu[uid] = 0, 0, 0\n",
    "    \n",
    "    # parcourt tweets\n",
    "    tweets.seek(0)\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        tweet = tweet.split()\n",
    "        uid, rtid = int(tweet[2]), int(tweet[-1])\n",
    "\n",
    "        # si tweet original update nb_tweets\n",
    "        if rtid == -1:\n",
    "            Lambda[uid] += 1\n",
    "\n",
    "        # si retweet update nb_retweets et nb_retweeted (si retweeted user connu)\n",
    "        else:\n",
    "            if cascade:\n",
    "                if rtid in LastPublisher:\n",
    "                    Mu[uid] += 1\n",
    "                    rtu = LastPublisher[rtid]\n",
    "                    Nu[rtu] += 1\n",
    "                    G.add_edge(rtu, uid)\n",
    "                else:\n",
    "                    Lambda[uid] += 1\n",
    "                LastPublisher[rtid] = uid\n",
    "            else:\n",
    "                if rtid in Author:\n",
    "                    Mu[uid] += 1\n",
    "                    rtu = Author[rtid]\n",
    "                    Nu[rtu] += 1\n",
    "                    G.add_edge(rtu, uid)\n",
    "                else:\n",
    "                    Author[rtid] = uid\n",
    "                    Lambda[uid] += 1\n",
    "\n",
    "        # on enregistre le ts du 1er tweet et celui du dernier tweet (update à chaque étape car tweets pas forcément pas ordre chrono)\n",
    "        ts = int(tweet[1])\n",
    "        if i==0:\n",
    "            first_ts = ts\n",
    "            last_ts = ts\n",
    "        elif ts < first_ts:\n",
    "            first_ts = ts\n",
    "        elif ts > last_ts:\n",
    "            last_ts = ts\n",
    "            \n",
    "\n",
    "total_time = last_ts - first_ts\n",
    "print(\"Tps ex : \", time()-start)\n",
    "tweets.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(users == set(Lambda.keys()))\n",
    "print(users == set(Mu.keys()))\n",
    "print(users == set(Nu.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each user u we divide `Lambda[u]`, `Mu[u]` and `Nu[u]` by `total_time` to get the final values for $\\lambda, \\mu, \\nu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in users:\n",
    "    Lambda[u] /= total_time\n",
    "    Mu[u] /= total_time\n",
    "    Nu[u] /= total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the main dictionary `MainDict` and add $\\lambda, \\mu, \\nu$ to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "MainDict = dict()\n",
    "for u in users:\n",
    "    MainDict[u] = {'lambda':Lambda[u], 'mu':Mu[u], 'nu':Nu[u]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete `Lambda`, `Mu` and `Nu` to save memory (they're not useful anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Lambda, Mu, Nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create user graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tps ex :  0.0\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "start = time()\n",
    "\n",
    "\n",
    "# si on étudie un vrai graphe (adjacency list)\n",
    "if truegraph:\n",
    "    for i,line in enumerate(open(adjacency_list, 'r')):\n",
    "        line = line.split()\n",
    "        G.add_edge(int(line[0]), int(line[1]))\n",
    "\n",
    "        \n",
    "# sinon on utilise une trace avec rtu, cascade ou rtid\n",
    "else:\n",
    "    \n",
    "    # get data\n",
    "    tweets = open(data_path, 'r')\n",
    "\n",
    "    # si on utilise des rtu\n",
    "    if RTU:\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            uid, rtu = int(tweet[2]), int(tweet[-1])\n",
    "            if rtu != -1 :\n",
    "                G.add_edge(rtu, uid)\n",
    "         \n",
    "        \n",
    "    # si on utilise cascade (avec rtid donc)\n",
    "    elif cascade:\n",
    "        \n",
    "        # last publisher dict\n",
    "        LastPublisher = dict()\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            twid, uid = int(tweet[0]), int(tweet[2])\n",
    "            LastPublisher[twid] = uid\n",
    "        \n",
    "        # create edges\n",
    "        tweets.seek(0)\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            uid, rtid = int(tweet[2]), int(tweet[-1])\n",
    "            if rtid != -1:\n",
    "                if rtid in LastPublisher:\n",
    "                    G.add_edge(LastPublisher[rtid], uid)\n",
    "                LastPublisher[rtid] = uid\n",
    "    \n",
    "    \n",
    "    # dernier cas : rtid simple (sans cascade)\n",
    "    else: \n",
    "        \n",
    "        # author dict\n",
    "        Author = dict()\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            twid, uid = int(tweet[0]), int(tweet[2])\n",
    "            Author[twid] = uid\n",
    "        \n",
    "        # create edges\n",
    "        tweets.seek(0)\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            uid, rtid = int(tweet[2]), int(tweet[-1])\n",
    "            if rtid != -1:\n",
    "                if rtid in Author:\n",
    "                    G.add_edge(Author[rtid], uid)\n",
    "                else:\n",
    "                    Author[rtid] = uid\n",
    "\n",
    "    \n",
    "    # close\n",
    "    tweets.close()\n",
    "\n",
    "# fin\n",
    "print(\"Tps ex : \", time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to `MainDict`:\n",
    "- in and out degrees\n",
    "- closeness, betweenness and Katz centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness = betweenness_centrality(G)\n",
    "\n",
    "for u in MainDict:\n",
    "    MainDict[u]['in_degree'] = G.in_degree[u]\n",
    "    MainDict[u]['out_degree'] = G.out_degree[u]\n",
    "    MainDict[u]['closeness_centrality'] = closeness_centrality(G, u)\n",
    "    MainDict[u]['betweenness_centrality'] = betweenness_centrality(G)[u]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute closeness centrality and add result to `MainDict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'lambda': 0.2,\n",
       "  'mu': 0.4,\n",
       "  'nu': 0.2,\n",
       "  'in_degree': 2,\n",
       "  'out_degree': 1,\n",
       "  'closeness_centrality': 0.6666666666666666,\n",
       "  'betweenness_centrality': 0.16666666666666666},\n",
       " 1: {'lambda': 0.2,\n",
       "  'mu': 0.4,\n",
       "  'nu': 0.0,\n",
       "  'in_degree': 2,\n",
       "  'out_degree': 0,\n",
       "  'closeness_centrality': 0.75,\n",
       "  'betweenness_centrality': 0.0},\n",
       " 2: {'lambda': 0.2,\n",
       "  'mu': 0.0,\n",
       "  'nu': 0.4,\n",
       "  'in_degree': 0,\n",
       "  'out_degree': 2,\n",
       "  'closeness_centrality': 0.0,\n",
       "  'betweenness_centrality': 0.0},\n",
       " 77: {'lambda': 0.0,\n",
       "  'mu': 0.0,\n",
       "  'nu': 0.2,\n",
       "  'in_degree': 0,\n",
       "  'out_degree': 1,\n",
       "  'closeness_centrality': 0.0,\n",
       "  'betweenness_centrality': 0.0}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MainDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
